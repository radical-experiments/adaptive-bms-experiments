__author__    = "Vivek Balasubramanian <vivek.balasubramanian@rutgers.edu>"
__copyright__ = "Copyright 2016, http://radical.rutgers.edu"
__license__   = "MIT"

from radical.entk import EoP, AppManager, Kernel, ResourceHandle
from pre_analysis import pre_analysis_kernel
from grompp import grompp_kernel
from mdrun import mdrun_kernel
from post_analysis import post_analysis_kernel
from check_convergence import check_convergence_kernel
import argparse, os, glob


## User settings
ENSEMBLE_SIZE=1    # Number of ensemble members / pipelines
PIPELINE_SIZE=4     # Number of stages in each pipeline, currently 4
TOTAL_ITERS=1       # Number of iterations to run current trial
SEED = 161536       # Seed for stage 1


## Please leave the following as is, issue #3 needs to be resolved before modifying these
## The following are helpful if we divide our entire experiment of N iterations 
## over M trials due to (say) walltime limitations
DONE=0              # Number of iterations already DONE
ITER=[(DONE+1) for x in range(1, ENSEMBLE_SIZE+1)]      # Iteration number to start with
DATA_LOC = ''       # Location where data from iterations 0 to DONE-1 is located


## Handling failed tasks
FAILED=[False for x in range(1, ENSEMBLE_SIZE+1)]       


class Test(EoP):

    def __init__(self, ensemble_size, pipeline_size):
        super(Test,self).__init__(ensemble_size, pipeline_size)

    def stage_1(self, instance):

        ## Stage 1 simply processes the parameter files to get started.
        ## Runs only for the first iteration

        global SEED

        k1 = Kernel(name="pre_analysis")
        k1.arguments = ["--template=CB7G3_template.mdp",
                        "--newname=CB7G3_run.mdp",
                        "--wldelta=2",
                        "--equilibrated=False",
                        "--lambda_state=0",
                        "--seed={0}".format(SEED)]
        k1.cores = 1

        k1.copy_input_data = ['$SHARED/CB7G3_template.mdp','$SHARED/analysis_1.py']

        return k1


    def stage_2(self, instance):

        ## Stage 2 is the gromacs preprocessing stage. In the first iteration, uses the output of stage 1. Otherwise
        ## operates over the .gro file from stage 3 of the previous iteration and .mdp file from stage 4 of the previous
        ## of iteration

        k2 = Kernel(name='grompp')
        k2.arguments = [  "--mdp=CB7G3_run.mdp",
                                "--conf=CB7G3.gro",
                                "--ndx=CB7G3.ndx",
                                "--top=CB7G3.top",
                                "--out=CB7G3.tpr"
                            ]        
        k2.cores=1


        k2.copy_input_data = [
                              '$SHARED/CB7G3.ndx',
                              '$SHARED/CB7G3.top',
                              '$SHARED/3atomtypes.itp',
                              '$SHARED/3_GMX.itp',
                              '$SHARED/cucurbit_7_uril_GMX.itp'
                            ]


        global ITER

        print 'iter: {0}, stage: 2 . instance: {1}'.format(ITER[instance-1], instance)

        if (ITER[instance-1]==DONE+1)or(FAILED[instance-1]==True):

            if DONE==0:
                k2.copy_input_data += ['$ITER_1_STAGE_1_TASK_{0}/CB7G3_run.mdp'.format(instance),
                                       '$SHARED/CB7G3.gro > CB7G3.gro']
            else:
                k2.copy_input_data += [ 
                                        '{2}/CB7G3_run{0}_gen{1}.mdp > CB7G3_run.mdp'.format(instance, DONE, DATA_LOC),
                                        '{2}/CB7G3_run{0}_gen{1}.gro > CB7G3.gro'.format(instance, DONE, DATA_LOC)]
            FAILED[instance-1] = False
        else:
            k2.copy_input_data += ['$ITER_{1}_STAGE_4_TASK_{0}/CB7G3_run.mdp'.format(instance, ITER[instance-1]-1-DONE),
                                   '$ITER_{1}_STAGE_3_TASK_{0}/CB7G3.gro'.format(instance, ITER[instance-1]-1-DONE)]

        return k2


    def branch_2(self, instance):

        global SEED

        ## If stage 2 fails, we simply rerun the current pipeline from scratch, i.e. stage 2. Stage 2 will then
        ## use the .mdp file from stage 1 but will keep naming convention consistent with the current iteration
        
        try:

            status = self.get_status(stage=2, instance=instance)
            if status == 'Failed':
                self.set_next_stage(stage=1)
                SEED += 1
                FAILED[instance-1] = True

        except Exception, ex:
            print 'Could not execute branch_2, error: ', ex


    def stage_3(self, instance):

        ## Stage 3 is the compute intensive gromacs mdrun stage. It operates over the .tpr file generated by stage 2.
        ## Stages its output (*.xvg, *.log) to shared location on remote -- 'staging_area' under the pilot folder. The 
        ## same output is also downloaded to the local machine to keep a backup.

        k3 = Kernel(name="mdrun")
        k3.arguments = [    "--nt=20",
                            "--tag=CB7G3",
                            "--out=CB7G3_dhdl.xvg"
                        ]
        k3.cores = 20
        k3.copy_input_data = ['$STAGE_2_TASK_{0}/CB7G3.tpr'.format(instance)]

        k3.copy_output_data = [ 'CB7G3_dhdl.xvg > $SHARED/CB7G3_run{1}_gen{0}_dhdl.xvg'.format(ITER[instance-1],instance),
                                'CB7G3_pullf.xvg > $SHARED/CB7G3_run{1}_gen{0}_pullf.xvg'.format(ITER[instance-1],instance),
                                'CB7G3_pullx.xvg > $SHARED/CB7G3_run{1}_gen{0}_pullx.xvg'.format(ITER[instance-1],instance),
                                'CB7G3.log > $SHARED/CB7G3_run{1}_gen{0}.log'.format(ITER[instance-1],instance)
                                ]

        k3.download_output_data = ['CB7G3.xtc > CB7G3_run{1}_gen{0}.xtc'.format(ITER[instance-1], instance),
                                    'CB7G3.log > CB7G3_run{1}_gen{0}.log'.format(ITER[instance-1],instance),
                                    'CB7G3_dhdl.xvg > CB7G3_run{1}_gen{0}_dhdl.xvg'.format(ITER[instance-1],instance),
                                    'CB7G3_pullf.xvg > CB7G3_run{1}_gen{0}_pullf.xvg'.format(ITER[instance-1],instance),
                                    'CB7G3_pullx.xvg > CB7G3_run{1}_gen{0}_pullx.xvg'.format(ITER[instance-1],instance),
                                    'CB7G3.gro > CB7G3_run{1}_gen{0}.gro'.format(ITER[instance-1], instance)
                                ]

        return k3


    def branch_3(self, instance):
        global ITER
        global SEED

        ## If stage 2 fails, we simply rerun the current pipeline from scratch, i.e. stage 2. Stage 2 will then
        ## use the .mdp file from stage 1 but will keep naming convention consistent with the current iteration

        try:
            status = self.get_status(stage=3, instance=instance)

            if status == 'Failed':

                self.set_next_stage(stage=2)
                print 'Task %s in stage 3 failed'%instance
                #ITER[instance-1]+=1
                SEED += 1

        except Exception, ex:
            print 'Could not execute branch_3, error: ', ex


    def stage_4(self, instance):

        ## Stage 4 executes the alchemical analysis script and prepares the .mdp file for the next iteration.
        ## It currently operates on all data (*.xvg, *.log) that is available at that moment in './data'. 
        ## './data' maps to the 'staging_area' that was referred to in stage 3. Downloads the results, output, error and
        ## the new mdp file to the local machine to keep a backup

        k4 = Kernel(name="post_analysis")
        k4.arguments = [    '--newname=CB7G3_run.mdp',
                            '--template=CB7G3_template.mdp',
                            '--dir=./data',
                            #'--prev_data=%s'%DATA_LOC
                            '--gen={0}'.format(ITER[instance - 1], instance),
                            '--run={1}'.format(ITER[instance - 1], instance)
                    ]

        k4.cores = 10

        k4.link_input_data = [  '$SHARED/analysis_2.py',
                                '$SHARED/alchemical_analysis.py',
                                '$SHARED/CB7G3_template.mdp',
                            ]

        if ITER[instance-1]>DONE+1:
            k4.link_input_data += ['$ITER_{1}_STAGE_4_TASK_{0}/analyze_1/results.txt > results_bak.txt'.format(instance, ITER[instance-1]-1-DONE)]

        k4.download_output_data = ['analyze_1/results.txt > results_run{1}_gen{0}.txt'.format(ITER[instance-1], instance),
                                    'STDOUT > stdout_run{1}_gen{0}'.format(ITER[instance-1], instance),
                                    'STDERR > stderr_run{1}_gen{0}'.format(ITER[instance-1], instance),
                                    'CB7G3_run.mdp > CB7G3_run{1}_gen{0}.mdp'.format(ITER[instance-1], instance),
                                    'results_average.txt > results_average_run{1}_gen{0}.txt'.format(ITER[instance - 1], instance),
                                    'data_total.txt > data_total_run{1}_gen{0}.txt'.format(ITER[instance - 1], instance)
                                    ]

        return k4

    def branch_4(self, instance):

        #convergence = self.get_output(stage=5,instance=instance)
        #print 'Convergence of pipeline: ', convergence

        global ITER
        global TOTAL_ITERS

        if ITER[instance-1]!=DONE+TOTAL_ITERS:
            ITER[instance-1]+=1
            self.set_next_stage(stage=2)            
        else:
            pass


        # if convergence > 1:
            #self.set_next_stage(stage=2)
        #else:
        #    pass





if __name__ == '__main__':

    # Create pattern object with desired ensemble size, pipeline size
    pipe = Test(ensemble_size=ENSEMBLE_SIZE, pipeline_size=PIPELINE_SIZE)

    # Create an application manager
    app = AppManager(name='Expanded_Ensemble', on_error='continue')

    # Register kernels to be used
    app.register_kernels(pre_analysis_kernel)
    app.register_kernels(grompp_kernel)
    app.register_kernels(mdrun_kernel)
    app.register_kernels(post_analysis_kernel)
    app.register_kernels(check_convergence_kernel)

    # Add workload to the application manager
    app.add_workload(pipe)

    # Download analysis file from MobleyLab repo
    os.system('curl -O https://raw.githubusercontent.com/MobleyLab/alchemical-analysis/master/alchemical_analysis/alchemical_analysis.py')

    # Parsing user cmd to get resource
    parser = argparse.ArgumentParser()
    parser.add_argument('--resource', help='target resource label')
    args = parser.parse_args()
    
    if args.resource != None:
        resource = args.resource
    else:
        resource = 'local.localhost'


    # Resource description to switch between resources
    res_dict = {

        'xsede.stampede': { 'cores': '272', 
                            'project': 'TG-MCB090174', 
                            'queue': 'normal', 
                            'path': '/home/vivek91/repos/expanded-ensemble'
                        },
        'xsede.comet': {    'cores': '288', 
                            'project': 'unc101', 
                            'queue': 'compute', 
                            'path': '/home/vivek/expanded-ensemble'
                        },
        'xsede.supermic': { 'cores': '340', 
                            'project':'TG-MCB090174', 
                            'queue':'workq'
                        }
        }

    # Create a resource handle for target machine
    res = ResourceHandle(resource=resource,
                cores=res_dict[resource]['cores'],
                username='trje3733',
                project = res_dict[resource]['project'],
                queue= res_dict[resource]['queue'],
                walltime=2400,
                database_url='mongodb://user:user@ds161194.mlab.com:61194/exp_2_multi_long_rep1',
                access_schema='gsissh'
                )

    # Data common to multiple tasks -- transferred only once to common staging area
    res.shared_data = [ './CB7G3.gro', './CB7G3.ndx', './CB7G3.top',
                        './CB7G3_template.mdp','./analysis_1.py',
                        './analysis_2.py', './determine_convergence.py',
                        './alchemical_analysis.py','./3atomtypes.itp',
                        './3_GMX.itp',
                        './cucurbit_7_uril_GMX.itp',
                        './CB7G3.gro']

    try:
        # Submit request for resources + wait till job becomes Active
        res.allocate(wait=True)

        # Run the given workload
        res.run(app)

    except Exception, ex:
        print 'Error, ',ex

    finally:
        # Deallocate the resource
        res.deallocate()
