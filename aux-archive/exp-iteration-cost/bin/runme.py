__author__    = "Vivek Balasubramanian <vivek.balasubramanian@rutgers.edu>"
__copyright__ = "Copyright 2016, http://radical.rutgers.edu"
__license__   = "MIT"

from radical.entk import EoP, AppManager, Kernel, ResourceHandle
from sleep import sleep_kernel
import argparse, os, glob


## User settings
PIPELINE_SIZE=4     # Number of stages in each pipeline, currently 4
class Test(EoP):

    def __init__(self, ensemble_size, pipeline_size):
        super(Test,self).__init__(ensemble_size, pipeline_size)

    def stage_1(self, instance):

        ## Stage 1 simply processes the parameter files to get started.
        ## Runs only for the first iteration

        k1 = Kernel(name="sleep")
        k1.arguments = ["--t=10"]
        k1.cores = 1

        return k1


    def stage_2(self, instance):

        ## Stage 2 is the gromacs preprocessing stage. In the first iteration, uses the output of stage 1. Otherwise
        ## operates over the .gro file from stage 3 of the previous iteration and .mdp file from stage 4 of the previous
        ## of iteration

        k2 = Kernel(name='sleep')
        k2.arguments = ["--t=10"]        
        k2.cores=1

        return k2


    def stage_3(self, instance):

        ## Stage 3 is the compute intensive gromacs mdrun stage. It operates over the .tpr file generated by stage 2.
        ## Stages its output (*.xvg, *.log) to shared location on remote -- 'staging_area' under the pilot folder. The 
        ## same output is also downloaded to the local machine to keep a backup.

        k3 = Kernel(name="sleep")
        k3.arguments = ["--t=10"]
        k3.cores = 1

        return k3

    def stage_4(self, instance):

        ## Stage 4 executes the alchemical analysis script and prepares the .mdp file for the next iteration.
        ## It currently operates on all data (*.xvg, *.log) that is available at that moment in './data'. 
        ## './data' maps to the 'staging_area' that was referred to in stage 3. Downloads the results, output, error and
        ## the new mdp file to the local machine to keep a backup

        k4 = Kernel(name="sleep")
        k4.arguments = ['--t=10']
        k4.cores = 1

        return k4

    def branch_4(self, instance):

        if ITER[instance-1]!=TOTAL_ITERS:
            ITER[instance-1]+=1
            self.set_next_stage(stage=1)            
        else:
            pass



if __name__ == '__main__':


    # Create an application manager
    app = AppManager(name='Expanded_Ensemble')

    # Register kernels to be used
    app.register_kernels(sleep_kernel)


    # Parsing user cmd to get resource
    parser = argparse.ArgumentParser()
    parser.add_argument('--resource', help='target resource label')
    parser.add_argument('--ensembles', help='number of pipelines')
    parser.add_argument('--iters', help='number of iterations')
    args = parser.parse_args()

    ENSEMBLE_SIZE = int(args.ensembles)
    TOTAL_ITERS = int(args.iters)
    ITER=[1 for x in range(1, ENSEMBLE_SIZE+1)]
 
    # Create pattern object with desired ensemble size, pipeline size
    pipe = Test(ensemble_size=ENSEMBLE_SIZE, pipeline_size=PIPELINE_SIZE)
    
    if args.resource != None:
        resource = args.resource
    else:
        resource = 'local.localhost'

    # Add workload to the application manager
    app.add_workload(pipe)

    # Resource description to switch between resources
    res_dict = {

        'xsede.comet': {    'cores': ENSEMBLE_SIZE+24, 
                            'project': 'unc101', 
                            'queue': 'compute', 
                        },
        'xsede.supermic': { 'cores': ENSEMBLE_SIZE+20, 
                            'project':'TG-MCB090174', 
                            'queue':'workq'
                        }
        }

    # Create a resource handle for target machine
    res = ResourceHandle(resource=resource,
                cores=res_dict[resource]['cores'],
                username='vivek91',
                project = res_dict[resource]['project'],
                queue= res_dict[resource]['queue'],
                walltime=10+1.5*TOTAL_ITERS,
                database_url='mongodb://user:user@ds239965.mlab.com:39965/admd-ee-sleep',
                access_schema='gsissh'
                )

    try:
        # Submit request for resources + wait till job becomes Active
        res.allocate(wait=True)

        # Run the given workload
        res.run(app)

    except Exception, ex:
        print 'Error, ',ex

    finally:
        # Deallocate the resource
        res.deallocate()
